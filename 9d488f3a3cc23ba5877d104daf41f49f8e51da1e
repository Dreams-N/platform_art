Revision: 9d488f3a3cc23ba5877d104daf41f49f8e51da1e
Patch-set: 1
File: /COMMIT_MSG

10:12-10:57
Mon Dec 07 23:57:44 2015 +0000
Author: Igor Murashkin <1021471@85c56323-6fa9-3386-8a01-6480fb634889>
UUID: 60e03ce1_87b64cae
Bytes: 182
Would you mind sharing the performance data for this? I would've expected (given a good load balance factor) that open addressing has 1 less cache miss on average than with chaining.

10:12-10:57
Tue Dec 08 16:40:02 2015 +0000
Author: Nikita Lazarev <1085353@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 60e03ce1_87b64cae
UUID: a02154e4_c8b2ef1e
Bytes: 1366
The performance data for the Google Maps application (if you need I can provide the same data for other applications).
The legend:
TOTAL - total amount of string comparisons for ALL FindIndex() invocations during the application launching.
AVG - average amount of string comparisons for ONE FindIndex() invocation during the application launching.
MAX - maximum amount of string comparisons for ONE FindIndex() invocations (the case of looking for a string which are located somewhere in the end of the chain and the chain is too long).

The results:
the Original solution:
TOTAL = 120683
AVG = 2.40462
MAX = 255 - there are several "very" long chains.

our solution (with the load_factor = 1):
TOTAL: 545
AVG: 0.06058
max = 1

Here you can see that there are very long collision chains (up to 255 elements) in the Original solution sometimes and we need to call the string comparison method each time. Meanwhile the longest chain in our table contains less amount of elements and only 1-2 of them are compared using the string comparison operation.

To measure it I added a counter inside the FindIndex() method initialized by zero and was incrementing it each time we needed to compare strings (each time the control reaches 'if (pred_(slot, element))' statement inside the while loop). Then I was collecting all the counter values for each FindIndex() invocation.

10:12-10:57
Tue Dec 08 17:58:32 2015 +0000
Author: Mathieu Chartier <1015378@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: a02154e4_c8b2ef1e
UUID: 80b290f4_2b89d1a0
Bytes: 213
Is that including this CL? The load factors we were using were too high, I adjusted them down and it provided a large reduction in time spent in the hash table:

https://android-review.googlesource.com/#/c/176895/

10:12-10:57
Thu Dec 10 11:40:34 2015 +0000
Author: Nikita Lazarev <1085353@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 80b290f4_2b89d1a0
UUID: 00430035_eb7031e5
Bytes: 3
Yes

13
Mon Dec 07 23:57:44 2015 +0000
Author: Igor Murashkin <1021471@85c56323-6fa9-3386-8a01-6480fb634889>
UUID: 60c91c79_1c937d7f
Bytes: 292
This would seem to indicate a lot of collisions today. Is there more detailed data?

Why not just fix the the string hash function to be produce more distributed results, resulting in less collisions?

(or in fact, since this is prezygote strings why not make this into a perfect hash table?)

13
Tue Dec 08 16:40:02 2015 +0000
Author: Nikita Lazarev <1085353@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 60c91c79_1c937d7f
UUID: 80be10e1_774dd0bf
Bytes: 768
Actually as I understand the hash function is good enough: the average amount of string comparisons for the Original version is about 2.4. It means the chains aren't long in general. But there are several long chains, appearing quite seldom. And the main issue here (in my opinion) is not the has function used, but the approach of collision resolving. I mean there can be (and there are in practice) a lot of already busy slots between the appropriate slot (the slot corresponding the hash) and the slot we are actually placing the element. And during looking up we need to visit all of these slots (because there aren't empty ones between them). This issue also appears when we are looking for an absent element: we need to pass all slots until we meet an empty one.

13
Mon Dec 14 22:16:49 2015 +0000
Author: Igor Murashkin <1021471@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: 80be10e1_774dd0bf
UUID: 65f20ea9_94980ce6
Bytes: 921
It just seems like there's plenty of low hanging fruit in the current implementation of hash_set:

1) Resizing should adjust the requested # of slots to the next nearest prime number, this will make it less likely that slot lookup (hashcode%slotindex) will collide for 2 distinct hashcodes.

-- This would potentially help when hash codes are different but the slots end up being the same, should help average case.

2) Linear probing might be inferior to quadratic probing or secondary hash function for giving back different buckets.

-- This would potentially help when the hash codes are the same, it's less likely to steal slots from another adjacent entry and reduce worst cases.

It would be interesting to explore these types of enhancements which would have much less complexity and also have a larger impact potential since we actually use the existing hash_set in much more places than just pre_zygote strings.

14
Mon Dec 07 23:57:44 2015 +0000
Author: Igor Murashkin <1021471@85c56323-6fa9-3386-8a01-6480fb634889>
UUID: a0af948e_e1d1bc20
Bytes: 238
In total or just the # of components? I see the default load factor is 1.0 which could account for the smaller size.

Given equivalent load factors I would expect the chained version to be larger given the need to store edge pointer data.

14
Tue Dec 08 16:40:02 2015 +0000
Author: Nikita Lazarev <1085353@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: a0af948e_e1d1bc20
UUID: 606d9c10_b909db76
Bytes: 404
I compared the value returned by the WriteToMemory(nullptr) method. Actually it's relevant with the num_buckets_ property.

You're totally right about the table size: if the load factors are the same the size is bigger. But I investigated that we can increase the load factor of our implementation to 1 without increasing of the average amount of string comparisons. And as the result - the size is less.

File: runtime/base/chain_hash_set.h

43:6-43:18
Tue Dec 08 17:58:32 2015 +0000
Author: Mathieu Chartier <1015378@85c56323-6fa9-3386-8a01-6480fb634889>
UUID: a0af948e_07016048
Bytes: 153
I wonder if it would be less code duplication to add a kChain template boolean to the normal hash set and just handle the places where the logic differs.

43:6-43:18
Thu Dec 10 11:40:34 2015 +0000
Author: Nikita Lazarev <1085353@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: a0af948e_07016048
UUID: a054f4e4_93651eca
Bytes: 615
I'm also not happy with the code duplication here. It can be several solutions.

1) Add a kChain template. There are two disadvantages here: first, the data structures are different (different class members, different data_ field structure) and second, we need to forbid using of some methods (like Insert()) for the Chain version.

2) To inherit the ChainHashSet (or it's better to rename it to ReadOnlyHashSet) class from the HashSet, changing some methods logic, deleting unnecessary ones. Also we can hide unnecessary class members under a private modifier.

I think it's better to implement the second variant.

43:6-43:18
Mon Dec 14 22:16:49 2015 +0000
Author: Igor Murashkin <1021471@85c56323-6fa9-3386-8a01-6480fb634889>
Parent: a054f4e4_93651eca
UUID: 852462f4_aacdefe1
Bytes: 978
I'm not entirely convinced yet we actually need a chain hash set at all. For that reason alone I wouldn't necessarily jump into cleaning this up right away.

If we did however need it, here's some ways we could clean it up:

----------

Two interfaces seems like they should be interchangeable (same public member functions), and anywhere the implementation is different it could simply be refactored into 2 separate classes (think of it as the strategy pattern):

  namespace detail {
   class OpenAddressingHashSetImpl <...> {...};
   class ChainHashSetImpl <...> { ... };
  }

Put them into the art::detail namespace (and into a separate detail\hash_set.h file, 2 files for each impl is also fine as you prefer) as per our convention when a templated class needs to have implementation-specific non-nested classes/functions.

Simply adding a new class that's a copy or just adding some 'if/else' cases to hash_set.h (to handle chaining) would lead to too much of a mess IMHO.

